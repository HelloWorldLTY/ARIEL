# Codes for paper: Towards Artificial Intelligence Research Assistant ü§î for Expert-Involved Learning

## Installation

To install the environment for querying LLM/LMM from API or Huggingface with local inference, please refer the yml file: **air.yml**. In short, installing [Huggingface](https://huggingface.co/) is enoughüòÑ.

To install the environment for evaluating the summary accuracy, please refer the yml file: **eval.yml**.

To finetune an LLM with LoRA, please refer the codes in [LLaMA-Factory](https://github.com/hiyouga/LLaMA-Factory).


## Datasets

Please visit our [Huggingface page](https://huggingface.co/datasets/iLOVE2D/AIR_DATA) for the access of datasets.

## Running

For the experiments of long document summarization, please refer the folder **text_eval**.

For the experiments of scientific figure understanding, please refer the folder **image_eval**.

For the experiments of self-verification and self-correction, please refer the folder **self_improve**.


For the experiments of multimodal hypothesis generation, please refer the folder **hypo_gen**.


## Acknowledgement

We thanküôá‚Äç the developers of the following softwares (platforms), and without their help, we cannot successfully conduct our research:

[OpenAI](https://platform.openai.com/), [Google](https://ai.google.dev/gemma), [Meta](https://www.llama.com/), [Mistral AI](https://mistral.ai/), [Zhipu AI](https://huggingface.co/THUDM/glm-4-9b-chat-hf), [Anthropic](https://www.anthropic.com/), [Alibaba](https://huggingface.co/Qwen/Qwen-VL-Chat), [Microsoft](https://www.microsoft.com/en-us/research/project/llava-large-language-and-vision-assistant/), [Huggingface](https://huggingface.co/), [LLM4BioHypoGen](https://github.com/TsinghuaC3I/LLM4BioHypoGen), and [LLaMA-Factory](https://github.com/hiyouga/LLaMA-Factory).



## Citation

```
@article{liu2025towards,
  title={Towards Artificial Intelligence Research Assistant for Expert-Involved Learning},
  author={Liu, Tianyu and Han, Simeng and Luo, Xiao and Wang, Hanchen and Lu, Pan and Zhu, Biqing and Wang, Yuge and Li, Keyi and Chen, Jiapeng and Qu, Rihao and others},
  journal={arXiv preprint arXiv:2505.04638},
  year={2025}
}
```
